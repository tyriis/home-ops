---
volumes:
  data:
    external: false
    name: ollama_data
    driver: local
services:
  ollama:
    scale: 1 # scale to 0 if you want to stop the service without losing data
    container_name: ollama
    environment:
      OLLAMA_MODELS: /data/.ollama/models
      # OLLAMA_KEEP_ALIVE: 24h
    # healthcheck:
    #   test:
    #     - CMD
    #     - curl
    #     - -f
    #     - http://localhost:11434/v1/models
    #   start_period: 15s
    #   interval: 30s
    #   timeout: 5s
    #   retries: 3
    image: ollama/ollama:0.16.3@sha256:63f052826604a28a83795df1a66d9e364410c0b8d1357a62a885c62cef05cdbb
    ports:
      - 11434:11434
    restart: unless-stopped
    volumes:
      - data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# TODO: find a way to pull the models f.e.
# llava:7b            8dd30f6b0cb1    4.7 GB    6 days ago
# gemma3:1b           8648f39daa8f    815 MB    6 days ago
# gemma3:4b           a2af6cc3eb7f    3.3 GB    8 days ago
# phi3:mini           4f2222927938    2.2 GB    8 days ago
# codestral:22b       0898a8b286d5    12 GB     9 days ago
